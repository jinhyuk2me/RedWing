#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TCN ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import precision_recall_fscore_support
import logging
from pathlib import Path
import json
from datetime import datetime

from config import TCN_CONFIG, GESTURE_CLASSES, GESTURE_CLASSES_KR, PATHS, TRAINING_CONFIG
from model import TCNGestureClassifier
from dataset import GestureDataset
from torch.utils.data import DataLoader

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelEvaluator:
    """TCN ëª¨ë¸ ì¢…í•© í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_path = model_path or PATHS['model_file']
        self.model = None
        self.test_loader = None
        
        # ê²°ê³¼ ì €ì¥ í´ë”
        self.results_dir = Path("evaluation_results")
        self.results_dir.mkdir(exist_ok=True)
        
    def load_model(self):
        """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            self.model = TCNGestureClassifier(
                input_size=TCN_CONFIG['input_size'],
                num_channels=TCN_CONFIG['num_channels'],
                kernel_size=TCN_CONFIG['kernel_size'],
                dropout=TCN_CONFIG['dropout'],
                num_classes=TCN_CONFIG['num_classes']
            )
            
            if Path(self.model_path).exists():
                checkpoint = torch.load(self.model_path, map_location=self.device)
                if 'model_state_dict' in checkpoint:
                    # checkpoint í˜•íƒœë¡œ ì €ì¥ëœ ê²½ìš°
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    # state_dictë§Œ ì €ì¥ëœ ê²½ìš°
                    self.model.load_state_dict(checkpoint)
                
                self.model.to(self.device)
                self.model.eval()
                logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
                return True
            else:
                logger.error(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
                return False
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def load_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        try:
            test_dataset = GestureDataset(
                data_root=PATHS['processed_data'],
                split='test',
                test_size=1.0 - TRAINING_CONFIG['train_test_split']
            )
            
            self.test_loader = DataLoader(
                test_dataset,
                batch_size=TRAINING_CONFIG['batch_size'],
                shuffle=False,
                num_workers=2
            )
            
            logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(test_dataset)}ê°œ ìƒ˜í”Œ")
            return True
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def evaluate_model(self):
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        if not self.model or not self.test_loader:
            logger.error("ëª¨ë¸ ë˜ëŠ” ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None
        
        all_predictions = []
        all_targets = []
        all_confidences = []
        
        with torch.no_grad():
            for batch_data, batch_targets in self.test_loader:
                batch_data = batch_data.to(self.device)
                batch_targets = batch_targets.to(self.device)
                
                # ì˜ˆì¸¡
                outputs = self.model(batch_data)
                probabilities = torch.softmax(outputs, dim=1)
                predictions = torch.argmax(outputs, dim=1)
                
                # ê²°ê³¼ ì €ì¥
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(batch_targets.cpu().numpy())
                all_confidences.extend(torch.max(probabilities, dim=1)[0].cpu().numpy())
        
        return {
            'predictions': np.array(all_predictions),
            'targets': np.array(all_targets),
            'confidences': np.array(all_confidences)
        }
    
    def calculate_metrics(self, results):
        """ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        predictions = results['predictions']
        targets = results['targets']
        confidences = results['confidences']
        
        # ê¸°ë³¸ ì§€í‘œ
        accuracy = accuracy_score(targets, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            targets, predictions, average='weighted'
        )
        
        # í´ë˜ìŠ¤ë³„ ì§€í‘œ
        class_report = classification_report(
            targets, predictions,
            target_names=[GESTURE_CLASSES_KR[i] for i in range(len(GESTURE_CLASSES))],
            output_dict=True
        )
        
        # ì‹ ë¢°ë„ í†µê³„
        confidence_stats = {
            'mean': np.mean(confidences),
            'std': np.std(confidences),
            'min': np.min(confidences),
            'max': np.max(confidences)
        }
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'class_report': class_report,
            'confidence_stats': confidence_stats
        }
    
    def plot_confusion_matrix(self, results, save_path=None):
        """Confusion Matrix ì‹œê°í™”"""
        predictions = results['predictions']
        targets = results['targets']
        
        # Confusion Matrix ê³„ì‚°
        cm = confusion_matrix(targets, predictions)
        
        # ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            cm, 
            annot=True, 
            fmt='d',
            cmap='Blues',
            xticklabels=[GESTURE_CLASSES_KR[i] for i in range(len(GESTURE_CLASSES))],
            yticklabels=[GESTURE_CLASSES_KR[i] for i in range(len(GESTURE_CLASSES))]
        )
        plt.title('ì œìŠ¤ì²˜ ì¸ì‹ Confusion Matrix', fontsize=14, fontweight='bold')
        plt.xlabel('ì˜ˆì¸¡ ì œìŠ¤ì²˜', fontsize=12)
        plt.ylabel('ì‹¤ì œ ì œìŠ¤ì²˜', fontsize=12)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Confusion Matrix ì €ì¥: {save_path}")
        
        plt.show()
    
    def plot_confidence_distribution(self, results, save_path=None):
        """ì‹ ë¢°ë„ ë¶„í¬ ì‹œê°í™”"""
        confidences = results['confidences']
        predictions = results['predictions']
        
        plt.figure(figsize=(12, 8))
        
        # ì „ì²´ ì‹ ë¢°ë„ íˆìŠ¤í† ê·¸ë¨
        plt.subplot(2, 2, 1)
        plt.hist(confidences, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        plt.title('ì „ì²´ ì‹ ë¢°ë„ ë¶„í¬')
        plt.xlabel('ì‹ ë¢°ë„')
        plt.ylabel('ë¹ˆë„')
        
        # í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ë°•ìŠ¤í”Œë¡¯
        plt.subplot(2, 2, 2)
        confidence_by_class = [confidences[predictions == i] for i in range(len(GESTURE_CLASSES))]
        plt.boxplot(confidence_by_class, labels=[GESTURE_CLASSES_KR[i] for i in range(len(GESTURE_CLASSES))])
        plt.title('í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ë¶„í¬')
        plt.ylabel('ì‹ ë¢°ë„')
        plt.xticks(rotation=45)
        
        # ì‹ ë¢°ë„ vs ì •í™•ë„
        plt.subplot(2, 2, 3)
        correct = (predictions == results['targets'])
        plt.scatter(confidences[correct], [1]*sum(correct), alpha=0.5, label='ì •ë‹µ', color='green')
        plt.scatter(confidences[~correct], [0]*sum(~correct), alpha=0.5, label='ì˜¤ë‹µ', color='red')
        plt.title('ì‹ ë¢°ë„ vs ì •ë‹µ/ì˜¤ë‹µ')
        plt.xlabel('ì‹ ë¢°ë„')
        plt.ylabel('ì •ë‹µ ì—¬ë¶€')
        plt.legend()
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ë³„ ì •í™•ë„
        plt.subplot(2, 2, 4)
        thresholds = np.arange(0.5, 1.0, 0.05)
        accuracies = []
        for threshold in thresholds:
            mask = confidences >= threshold
            if sum(mask) > 0:
                acc = accuracy_score(results['targets'][mask], predictions[mask])
                accuracies.append(acc)
            else:
                accuracies.append(0)
        
        plt.plot(thresholds, accuracies, marker='o')
        plt.title('ì‹ ë¢°ë„ ì„ê³„ê°’ë³„ ì •í™•ë„')
        plt.xlabel('ì‹ ë¢°ë„ ì„ê³„ê°’')
        plt.ylabel('ì •í™•ë„')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ì‹ ë¢°ë„ ë¶„í¬ ì €ì¥: {save_path}")
        
        plt.show()
    
    def save_evaluation_report(self, metrics, save_path=None):
        """í‰ê°€ ë³´ê³ ì„œ ì €ì¥"""
        if not save_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = self.results_dir / f"evaluation_report_{timestamp}.json"
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'model_path': str(self.model_path),
            'overall_metrics': {
                'accuracy': float(metrics['accuracy']),
                'precision': float(metrics['precision']),
                'recall': float(metrics['recall']),
                'f1_score': float(metrics['f1_score'])
            },
            'confidence_stats': metrics['confidence_stats'],
            'class_report': metrics['class_report']
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"í‰ê°€ ë³´ê³ ì„œ ì €ì¥: {save_path}")
        return save_path
    
    def run_full_evaluation(self):
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        logger.info("=== TCN ëª¨ë¸ ì¢…í•© í‰ê°€ ì‹œì‘ ===")
        
        # 1. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
        if not self.load_model():
            return False
        
        if not self.load_test_data():
            return False
        
        # 2. ëª¨ë¸ í‰ê°€
        logger.info("ëª¨ë¸ í‰ê°€ ì¤‘...")
        results = self.evaluate_model()
        if not results:
            return False
        
        # 3. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        logger.info("ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì¤‘...")
        metrics = self.calculate_metrics(results)
        
        # 4. ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ“Š TCN ëª¨ë¸ í‰ê°€ ê²°ê³¼")
        print("="*50)
        print(f"âœ… ì „ì²´ ì •í™•ë„: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
        print(f"ğŸ“ˆ ì •ë°€ë„: {metrics['precision']:.4f}")
        print(f"ğŸ“‰ ì¬í˜„ìœ¨: {metrics['recall']:.4f}")
        print(f"ğŸ¯ F1-Score: {metrics['f1_score']:.4f}")
        print(f"ğŸ” í‰ê·  ì‹ ë¢°ë„: {metrics['confidence_stats']['mean']:.4f}")
        print("="*50)
        
        # 5. ì‹œê°í™”
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Confusion Matrix
        cm_path = self.results_dir / f"confusion_matrix_{timestamp}.png"
        self.plot_confusion_matrix(results, cm_path)
        
        # ì‹ ë¢°ë„ ë¶„í¬
        conf_path = self.results_dir / f"confidence_distribution_{timestamp}.png"
        self.plot_confidence_distribution(results, conf_path)
        
        # 6. ë³´ê³ ì„œ ì €ì¥
        report_path = self.save_evaluation_report(metrics)
        
        logger.info("=== í‰ê°€ ì™„ë£Œ ===")
        return True

if __name__ == "__main__":
    evaluator = ModelEvaluator()
    evaluator.run_full_evaluation() 