#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‹œê°ì  ë°ëª¨ ì˜ìƒ ìƒì„±
í•™ìŠµëœ TCN ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜¤ë²„ë ˆì´í•˜ëŠ” ë°ëª¨ ì˜ìƒ ìƒì„±
"""

import cv2
import numpy as np
import torch
import json
import mediapipe as mp
from pathlib import Path
from collections import deque
from datetime import datetime

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import GESTURE_CLASSES, MEDIAPIPE_CONFIG
from model import GestureModelManager
from utils import setup_logging

class VisualDemoCreator:
    """ì‹œê°ì  ë°ëª¨ ì˜ìƒ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.logger = setup_logging()
        
        # MediaPipe ì´ˆê¸°í™”
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        
        # ëª¨ë¸ ë¡œë“œ
        self.model_manager = GestureModelManager()
        self.model = self.model_manager.load_model()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        self.model.eval()
        
        # ìì„¸ ë²„í¼
        self.pose_buffer = deque(maxlen=30)
        self.key_landmarks = MEDIAPIPE_CONFIG['key_landmarks']
        
        # ìƒ‰ìƒ ì •ì˜
        self.colors = {
            'stop': (0, 0, 255),      # Red
            'forward': (0, 255, 0),   # Green  
            'left': (255, 0, 0),      # Blue
            'right': (0, 255, 255),   # Yellow
            'background': (0, 0, 0),   # Black
            'text': (255, 255, 255),   # White
            'skeleton': (255, 128, 0)  # Orange
        }
        
        self.logger.info("ì‹œê°ì  ë°ëª¨ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_pose_landmarks(self, frame):
        """ìì„¸ ì¶”ì¶œ (í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ ë°©ì‹)"""
        height, width = frame.shape[:2]
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        rgb_frame.flags.writeable = False
        
        results = self.pose.process(rgb_frame)
        
        if results.pose_landmarks:
            pose_data = []
            for idx in self.key_landmarks:
                landmark = results.pose_landmarks.landmark[idx]
                pose_data.append([landmark.x, landmark.y, landmark.visibility])
            
            return np.array(pose_data, dtype=np.float32), results
        
        return None, results
    
    def normalize_pose_data(self, pose_data):
        """ì •ê·œí™” (í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ ë°©ì‹)"""
        if pose_data.shape[0] == 0:
            return pose_data
            
        normalized_pose = pose_data.copy()
        
        # Hip ì¤‘ì‹¬ì  ê³„ì‚°
        left_hip = pose_data[9]
        right_hip = pose_data[10]
        
        if left_hip[2] > 0.5 and right_hip[2] > 0.5:
            center = (left_hip[:2] + right_hip[:2]) / 2
            
            # ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜
            for joint_idx in range(len(self.key_landmarks)):
                if pose_data[joint_idx][2] > 0.5:
                    normalized_pose[joint_idx][:2] -= center
            
            # ìŠ¤ì¼€ì¼ ì •ê·œí™”
            left_shoulder = pose_data[3]
            right_shoulder = pose_data[4]
            
            if left_shoulder[2] > 0.5 and right_shoulder[2] > 0.5:
                shoulder_width = np.linalg.norm(left_shoulder[:2] - right_shoulder[:2])
                if shoulder_width > 0:
                    normalized_pose[:, :2] /= shoulder_width
        
        return normalized_pose
    
    def predict_gesture(self, pose_sequence):
        """ì œìŠ¤ì²˜ ì˜ˆì¸¡"""
        if len(pose_sequence) < 30:
            return None, 0.0
        
        # ìµœê·¼ 30í”„ë ˆì„ ì‚¬ìš©
        input_sequence = np.array(pose_sequence[-30:])  # (30, 17, 3)
        input_sequence = input_sequence[:, :, :2]  # (30, 17, 2) - x,yë§Œ
        input_sequence = input_sequence.reshape(input_sequence.shape[0], -1)  # (30, 34)
        
        # ì˜ˆì¸¡
        input_tensor = torch.FloatTensor(input_sequence).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            output = self.model(input_tensor)
            probabilities = torch.softmax(output, dim=1)
            confidence, predicted_class = torch.max(probabilities, 1)
            
            predicted_gesture = GESTURE_CLASSES[predicted_class.item()]
            confidence_score = confidence.item()
        
        return predicted_gesture, confidence_score
    
    def draw_enhanced_overlay(self, frame, gt_gesture, prediction, confidence, frame_info, pose_results):
        """í–¥ìƒëœ ì‹œê°ì  ì˜¤ë²„ë ˆì´"""
        height, width = frame.shape[:2]
        
        # ë°˜íˆ¬ëª… ì˜¤ë²„ë ˆì´ íŒ¨ë„ ìƒì„±
        overlay = frame.copy()
        
        # ìƒë‹¨ ì •ë³´ íŒ¨ë„ (ë°˜íˆ¬ëª… ê²€ì€ ë°°ê²½)
        cv2.rectangle(overlay, (0, 0), (width, 120), self.colors['background'], -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # í•˜ë‹¨ ì •ë³´ íŒ¨ë„
        cv2.rectangle(overlay, (0, height-80), (width, height), self.colors['background'], -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # ìì„¸ ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸° (ê°œì„ ëœ ìŠ¤íƒ€ì¼)
        if pose_results.pose_landmarks:
            # ìŠ¤ì¼ˆë ˆí†¤ì„ ë” êµµê³  ëˆˆì— ë„ê²Œ
            self.mp_drawing.draw_landmarks(
                frame, 
                pose_results.pose_landmarks, 
                self.mp_pose.POSE_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing.DrawingSpec(
                    color=self.colors['skeleton'], thickness=4, circle_radius=6),
                connection_drawing_spec=self.mp_drawing.DrawingSpec(
                    color=self.colors['skeleton'], thickness=3)
            )
        
        # Ground Truth (ì™¼ìª½ ìƒë‹¨)
        if gt_gesture:
            gt_color = self.colors[gt_gesture]
            cv2.rectangle(frame, (10, 10), (300, 50), gt_color, 3)
            cv2.putText(frame, f'GROUND TRUTH: {gt_gesture.upper()}', (20, 35), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, gt_color, 2)
        
        # Prediction (ì˜¤ë¥¸ìª½ ìƒë‹¨)
        if prediction:
            pred_color = self.colors[prediction]
            cv2.rectangle(frame, (width-350, 10), (width-10, 50), pred_color, 3)
            cv2.putText(frame, f'PREDICTION: {prediction.upper()}', (width-340, 35), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, pred_color, 2)
            
            # ì‹ ë¢°ë„ ë°” (ì˜¤ë¥¸ìª½ ìƒë‹¨)
            bar_width = int(300 * confidence)
            cv2.rectangle(frame, (width-350, 60), (width-50, 85), (64, 64, 64), -1)
            cv2.rectangle(frame, (width-350, 60), (width-350+bar_width, 85), pred_color, -1)
            cv2.putText(frame, f'Confidence: {confidence:.1%}', (width-340, 100), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['text'], 2)
        
        # ì •í™•ì„± í‘œì‹œ (ì¤‘ì•™ ìƒë‹¨)
        if prediction and gt_gesture:
            is_correct = prediction == gt_gesture
            status_color = (0, 255, 0) if is_correct else (0, 0, 255)
            status_text = "CORRECT" if is_correct else "WRONG"
            
            # í° ìƒíƒœ í‘œì‹œ
            cv2.rectangle(frame, (width//2-100, 10), (width//2+100, 50), status_color, 3)
            cv2.putText(frame, status_text, (width//2-80, 35), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)
        
        # í”„ë ˆì„ ì •ë³´ (í•˜ë‹¨ ì™¼ìª½)
        frame_text = f"Frame: {frame_info['current']}/{frame_info['total']} ({frame_info['progress']:.1f}%)"
        cv2.putText(frame, frame_text, (10, height-50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['text'], 2)
        
        # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ (í•˜ë‹¨ ì¤‘ì•™)
        if 'segment' in frame_info:
            segment_text = f"Segment: {frame_info['segment']['gesture']} ({frame_info['segment']['progress']:.0f}%)"
            cv2.putText(frame, segment_text, (width//2-100, height-50), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['text'], 2)
        
        # ëª¨ë¸ ì •ë³´ (í•˜ë‹¨ ì˜¤ë¥¸ìª½)
        cv2.putText(frame, "TCN Model", (width-150, height-50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['text'], 2)
        cv2.putText(frame, "89.6% Accuracy", (width-150, height-25), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'], 1)
        
        return frame
    
    def create_visual_demo(self, demo_path: str, segments_path: str):
        """ì‹œê°ì  ë°ëª¨ ì˜ìƒ ìƒì„±"""
        
        # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ë¡œë“œ
        with open(segments_path, 'r', encoding='utf-8') as f:
            segments = json.load(f)
        
        self.logger.info(f"ì‹œê°ì  ë°ëª¨ ì˜ìƒ ìƒì„± ì‹œì‘: {demo_path}")
        
        cap = cv2.VideoCapture(demo_path)
        
        # ë¹„ë””ì˜¤ ì†ì„±
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # ì¶œë ¥ ì˜ìƒ ì„¤ì •
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = f'demo_videos/visual_demo_{timestamp}.mp4'
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        frame_idx = 0
        correct_count = 0
        total_predictions = 0
        
        self.logger.info("ì‹œê°ì  ì˜¤ë²„ë ˆì´ ì˜ìƒ ìƒì„± ì¤‘...")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # í˜„ì¬ í”„ë ˆì„ì˜ ground truth ì°¾ê¸°
            current_gt = None
            current_segment = None
            for segment in segments:
                if segment['start_frame'] <= frame_idx <= segment['end_frame']:
                    current_gt = segment['gesture']
                    current_segment = segment
                    break
            
            # ìì„¸ ì¶”ì • ë° ì˜ˆì¸¡
            pose_data, pose_results = self.extract_pose_landmarks(frame)
            
            prediction = None
            confidence = 0.0
            
            if pose_data is not None:
                normalized_pose = self.normalize_pose_data(pose_data)
                self.pose_buffer.append(normalized_pose)
                
                if len(self.pose_buffer) == 30:
                    prediction, confidence = self.predict_gesture(list(self.pose_buffer))
            
            # í”„ë ˆì„ ì •ë³´ êµ¬ì„±
            frame_info = {
                'current': frame_idx,
                'total': total_frames,
                'progress': (frame_idx / total_frames) * 100
            }
            
            if current_segment:
                segment_progress = ((frame_idx - current_segment['start_frame']) / 
                                  (current_segment['end_frame'] - current_segment['start_frame'])) * 100
                frame_info['segment'] = {
                    'gesture': current_segment['gesture'],
                    'progress': segment_progress
                }
            
            # ì •í™•ë„ ì¶”ì 
            if prediction and current_gt:
                total_predictions += 1
                if prediction == current_gt:
                    correct_count += 1
            
            # ì‹œê°ì  ì˜¤ë²„ë ˆì´ ì ìš©
            visual_frame = self.draw_enhanced_overlay(
                frame, current_gt, prediction, confidence, frame_info, pose_results)
            
            out.write(visual_frame)
            
            # ì§„í–‰ ìƒí™© ë¡œê·¸
            if frame_idx % 100 == 0:
                current_accuracy = (correct_count / total_predictions * 100) if total_predictions > 0 else 0
                self.logger.info(f"  í”„ë ˆì„ {frame_idx}/{total_frames} ({frame_info['progress']:.1f}%) - í˜„ì¬ ì •í™•ë„: {current_accuracy:.1f}%")
            
            frame_idx += 1
        
        cap.release()
        out.release()
        
        # ìµœì¢… ê²°ê³¼
        final_accuracy = (correct_count / total_predictions * 100) if total_predictions > 0 else 0
        
        self.logger.info(f"\nğŸ¬ ì‹œê°ì  ë°ëª¨ ì˜ìƒ ìƒì„± ì™„ë£Œ!")
        self.logger.info(f"ğŸ“¹ ì¶œë ¥ íŒŒì¼: {output_path}")
        self.logger.info(f"ğŸ¯ ìµœì¢… ì •í™•ë„: {final_accuracy:.2f}% ({correct_count}/{total_predictions})")
        self.logger.info(f"ğŸ“Š ì´ í”„ë ˆì„: {total_frames}ê°œ")
        
        return output_path, final_accuracy

if __name__ == "__main__":
    # ê°€ì¥ ìµœê·¼ ë°ëª¨ ì˜ìƒ ì°¾ê¸°
    demo_dir = Path('demo_videos')
    if demo_dir.exists():
        demo_files = list(demo_dir.glob('concatenated_demo_*.mp4'))
        if demo_files:
            # ê°€ì¥ ìµœê·¼ íŒŒì¼
            latest_demo = sorted(demo_files)[-1]
            segments_file = str(latest_demo).replace('.mp4', '_segments.json')
            
            if Path(segments_file).exists():
                print(f"ğŸ¬ ì‹œê°ì  ë°ëª¨ ì˜ìƒ ìƒì„± ì‹œì‘: {latest_demo.name}")
                
                creator = VisualDemoCreator()
                output_path, accuracy = creator.create_visual_demo(str(latest_demo), segments_file)
                
                print(f"\nâœ… ì‹œê°ì  ë°ëª¨ ìƒì„± ì™„ë£Œ!")
                print(f"ğŸ“¹ ê²°ê³¼ ì˜ìƒ: {output_path}")
                print(f"ğŸ¯ ì •í™•ë„: {accuracy:.2f}%")
                print(f"\nğŸ¥ ì˜ìƒì„ ì¬ìƒí•´ì„œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
            else:
                print(f"âŒ ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {segments_file}")
        else:
            print("âŒ ë°ëª¨ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ demo_videos í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.") 