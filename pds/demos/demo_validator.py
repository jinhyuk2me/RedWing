#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°ëª¨ ì˜ìƒ ìƒì„± ë° ì‹¤ì‹œê°„ ëª¨ë¸ ê²€ì¦
ëœë¤í•˜ê²Œ ì„ íƒëœ ì œìŠ¤ì²˜ ì˜ìƒë“¤ì„ ì´ì–´ë¶™ì—¬ì„œ ì—°ì†ì ì¸ ë°ëª¨ë¥¼ ë§Œë“¤ê³ ,
í•™ìŠµëœ TCN ëª¨ë¸ë¡œ ì‹¤ì‹œê°„ ì˜ˆì¸¡í•˜ì—¬ ì„±ëŠ¥ì„ ê²€ì¦
"""

import cv2
import numpy as np
import torch
import random
import os
from pathlib import Path
from collections import deque
import time
from datetime import datetime

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import GESTURE_CLASSES, MEDIAPIPE_CONFIG, PATHS
from model import GestureModelManager
from pose_estimator import RealTimePoseDetector
from utils import setup_logging

class DemoVideoCreator:
    """ë°ëª¨ ì˜ìƒ ìƒì„± ë° ì‹¤ì‹œê°„ ê²€ì¦"""
    
    def __init__(self):
        self.logger = setup_logging()
        self.pose_data_path = Path('../pose_data_rotated')
        self.gesture_classes = GESTURE_CLASSES
        self.gesture_colors = {
            'stop': (0, 0, 255),      # Red
            'forward': (0, 255, 0),   # Green
            'left': (255, 0, 0),      # Blue
            'right': (0, 255, 255)    # Yellow
        }
        
        # ëª¨ë¸ ë¡œë“œ
        self.model_manager = GestureModelManager()
        self.model = self.model_manager.load_model()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        self.model.eval()
        
        # ìì„¸ ì¶”ì •ê¸°
        self.pose_estimator = RealTimePoseDetector()
        
        # ì‹¤ì‹œê°„ ì˜ˆì¸¡ì„ ìœ„í•œ ë²„í¼
        self.pose_buffer = deque(maxlen=30)  # 30í”„ë ˆì„ = 1ì´ˆ
        
        self.logger.info("ë°ëª¨ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def select_random_videos(self, videos_per_gesture: int = 3) -> list:
        """ê° ì œìŠ¤ì²˜ë³„ë¡œ ëœë¤í•˜ê²Œ ì˜ìƒ ì„ íƒ"""
        selected_videos = []
        
        for gesture_name in self.gesture_classes.values():
            gesture_folder = self.pose_data_path / gesture_name
            
            if not gesture_folder.exists():
                self.logger.warning(f"{gesture_name} í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {gesture_folder}")
                continue
            
            # í•´ë‹¹ ì œìŠ¤ì²˜ì˜ ëª¨ë“  ì˜ìƒ íŒŒì¼
            video_files = list(gesture_folder.glob("*.mp4"))
            
            if len(video_files) < videos_per_gesture:
                self.logger.warning(f"{gesture_name}ì— ì¶©ë¶„í•œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤. {len(video_files)}ê°œë§Œ ì‚¬ìš©")
                selected = video_files
            else:
                selected = random.sample(video_files, videos_per_gesture)
            
            for video_file in selected:
                selected_videos.append({
                    'path': video_file,
                    'gesture': gesture_name,
                    'expected_label': list(self.gesture_classes.keys())[list(self.gesture_classes.values()).index(gesture_name)]
                })
        
        # ëœë¤í•˜ê²Œ ì„ê¸°
        random.shuffle(selected_videos)
        
        self.logger.info(f"ì´ {len(selected_videos)}ê°œ ì˜ìƒ ì„ íƒë¨")
        for video in selected_videos:
            self.logger.info(f"  - {video['gesture']}: {video['path'].name}")
        
        return selected_videos
    
    def create_concatenated_demo(self, selected_videos: list, output_path: str = None) -> str:
        """ì„ íƒëœ ì˜ìƒë“¤ì„ ì´ì–´ë¶™ì—¬ì„œ ë°ëª¨ ì˜ìƒ ìƒì„±"""
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = f'demo_videos/concatenated_demo_{timestamp}.mp4'
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('demo_videos', exist_ok=True)
        
        # ì²« ë²ˆì§¸ ì˜ìƒì˜ ì†ì„± í™•ì¸
        cap = cv2.VideoCapture(str(selected_videos[0]['path']))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        cap.release()
        
        # ë¹„ë””ì˜¤ ë¼ì´í„° ì„¤ì •
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # ê° ì˜ìƒë³„ ì •ë³´ ì €ì¥ (ë‚˜ì¤‘ì— ì˜ˆì¸¡ ê²°ê³¼ì™€ ë¹„êµìš©)
        video_segments = []
        frame_count = 0
        
        self.logger.info("ì˜ìƒ ì´ì–´ë¶™ì´ê¸° ì‹œì‘...")
        
        for i, video_info in enumerate(selected_videos):
            video_path = video_info['path']
            gesture = video_info['gesture']
            
            cap = cv2.VideoCapture(str(video_path))
            segment_start = frame_count
            
            self.logger.info(f"  [{i+1}/{len(selected_videos)}] {gesture}: {video_path.name}")
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # í”„ë ˆì„ì— ì œìŠ¤ì²˜ ì •ë³´ ì˜¤ë²„ë ˆì´ (ground truth)
                color = self.gesture_colors[gesture]
                cv2.putText(frame, f'GT: {gesture.upper()}', (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
                
                out.write(frame)
                frame_count += 1
            
            cap.release()
            
            video_segments.append({
                'gesture': gesture,
                'start_frame': segment_start,
                'end_frame': frame_count - 1,
                'video_file': video_path.name
            })
        
        out.release()
        
        self.logger.info(f"ë°ëª¨ ì˜ìƒ ìƒì„± ì™„ë£Œ: {output_path}")
        self.logger.info(f"ì´ í”„ë ˆì„ ìˆ˜: {frame_count}")
        
        # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ì €ì¥
        segments_info_path = output_path.replace('.mp4', '_segments.json')
        import json
        with open(segments_info_path, 'w', encoding='utf-8') as f:
            json.dump(video_segments, f, indent=2, ensure_ascii=False)
        
        return output_path, video_segments
    
    def predict_pose_sequence(self, pose_sequence: np.ndarray) -> tuple:
        """ìì„¸ ì‹œí€€ìŠ¤ì— ëŒ€í•œ TCN ëª¨ë¸ ì˜ˆì¸¡"""
        if len(pose_sequence) < 30:
            return None, 0.0
        
        # ìµœê·¼ 30í”„ë ˆì„ ì‚¬ìš©
        input_sequence = pose_sequence[-30:]  # (30, 17, 2)
        
        # í…ì„œë¡œ ë³€í™˜
        input_tensor = torch.FloatTensor(input_sequence).unsqueeze(0).to(self.device)  # (1, 30, 17, 2)
        
        with torch.no_grad():
            output = self.model(input_tensor)
            probabilities = torch.softmax(output, dim=1)
            confidence, predicted_class = torch.max(probabilities, 1)
            
            predicted_gesture = self.gesture_classes[predicted_class.item()]
            confidence_score = confidence.item()
        
        return predicted_gesture, confidence_score
    
    def run_realtime_validation(self, demo_video_path: str, video_segments: list):
        """ë°ëª¨ ì˜ìƒì— ëŒ€í•œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ë° ê²€ì¦"""
        self.logger.info("ì‹¤ì‹œê°„ ê²€ì¦ ì‹œì‘...")
        
        cap = cv2.VideoCapture(demo_video_path)
        
        # ê²°ê³¼ ì €ì¥ìš©
        results = []
        frame_idx = 0
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì˜ìƒ ì €ì¥
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        
        output_path = demo_video_path.replace('.mp4', '_with_predictions.mp4')
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        self.logger.info("í”„ë ˆì„ë³„ ì˜ˆì¸¡ ì‹œì‘...")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # í˜„ì¬ í”„ë ˆì„ì˜ ground truth ì°¾ê¸°
            current_gt = None
            for segment in video_segments:
                if segment['start_frame'] <= frame_idx <= segment['end_frame']:
                    current_gt = segment['gesture']
                    break
            
            # ìì„¸ ì¶”ì •
            pose_landmarks = self.pose_estimator.process_frame(frame)
            
            prediction = None
            confidence = 0.0
            
            if pose_landmarks is not None:
                # ì •ê·œí™”ëœ ìì„¸ ë°ì´í„° ì¶”ì¶œ
                pose_data = self.pose_estimator.extract_keypoints(pose_landmarks)
                self.pose_buffer.append(pose_data)
                
                # ì¶©ë¶„í•œ í”„ë ˆì„ì´ ëª¨ì´ë©´ ì˜ˆì¸¡
                if len(self.pose_buffer) == 30:
                    pose_sequence = np.array(list(self.pose_buffer))
                    prediction, confidence = self.predict_pose_sequence(pose_sequence)
            
            # ê²°ê³¼ ê¸°ë¡
            result = {
                'frame': frame_idx,
                'ground_truth': current_gt,
                'prediction': prediction,
                'confidence': confidence,
                'correct': prediction == current_gt if prediction else False
            }
            results.append(result)
            
            # í”„ë ˆì„ì— ì˜ˆì¸¡ ê²°ê³¼ ì˜¤ë²„ë ˆì´
            display_frame = frame.copy()
            
            # Ground Truth (ìœ„ìª½)
            if current_gt:
                gt_color = self.gesture_colors[current_gt]
                cv2.putText(display_frame, f'GT: {current_gt.upper()}', (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, gt_color, 2)
            
            # Prediction (ì•„ë˜ìª½)
            if prediction:
                pred_color = self.gesture_colors[prediction]
                cv2.putText(display_frame, f'PRED: {prediction.upper()} ({confidence:.2f})', 
                           (10, height - 40), cv2.FONT_HERSHEY_SIMPLEX, 1, pred_color, 2)
                
                # ì •í™•ì„± í‘œì‹œ
                if result['correct']:
                    cv2.putText(display_frame, 'CORRECT', (width - 150, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                else:
                    cv2.putText(display_frame, 'WRONG', (width - 120, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            # í”„ë ˆì„ ë²ˆí˜¸
            cv2.putText(display_frame, f'Frame: {frame_idx}', (10, height - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            out.write(display_frame)
            
            if frame_idx % 100 == 0:
                self.logger.info(f"  ì²˜ë¦¬ëœ í”„ë ˆì„: {frame_idx}")
            
            frame_idx += 1
        
        cap.release()
        out.release()
        
        # ê²°ê³¼ ë¶„ì„
        self.analyze_results(results, demo_video_path, video_segments)
        
        self.logger.info(f"ì˜ˆì¸¡ ê²°ê³¼ ì˜ìƒ ì €ì¥: {output_path}")
        
        return results, output_path
    
    def analyze_results(self, results: list, demo_video_path: str, video_segments: list):
        """ê²€ì¦ ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“Š ì‹¤ì‹œê°„ ê²€ì¦ ê²°ê³¼ ë¶„ì„")
        self.logger.info("="*60)
        
        # ì „ì²´ ì •í™•ë„
        total_frames = len([r for r in results if r['prediction'] is not None])
        correct_predictions = len([r for r in results if r['correct']])
        
        if total_frames > 0:
            overall_accuracy = correct_predictions / total_frames
            self.logger.info(f"ğŸ¯ ì „ì²´ ì •í™•ë„: {overall_accuracy:.4f} ({correct_predictions}/{total_frames})")
        
        # ì œìŠ¤ì²˜ë³„ ì •í™•ë„
        self.logger.info("\nğŸ“ˆ ì œìŠ¤ì²˜ë³„ ì„±ëŠ¥:")
        for gesture in self.gesture_classes.values():
            gesture_results = [r for r in results if r['ground_truth'] == gesture and r['prediction'] is not None]
            if gesture_results:
                gesture_correct = len([r for r in gesture_results if r['correct']])
                gesture_accuracy = gesture_correct / len(gesture_results)
                avg_confidence = np.mean([r['confidence'] for r in gesture_results])
                
                self.logger.info(f"  {gesture:8s}: {gesture_accuracy:.4f} ({gesture_correct:3d}/{len(gesture_results):3d}) | í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
        
        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¶„ì„
        self.logger.info("\nğŸ“‹ ì˜ìƒ ì„¸ê·¸ë¨¼íŠ¸ë³„ ì„±ëŠ¥:")
        for i, segment in enumerate(video_segments):
            segment_results = [r for r in results 
                             if segment['start_frame'] <= r['frame'] <= segment['end_frame'] 
                             and r['prediction'] is not None]
            
            if segment_results:
                segment_correct = len([r for r in segment_results if r['correct']])
                segment_accuracy = segment_correct / len(segment_results)
                
                self.logger.info(f"  [{i+1:2d}] {segment['gesture']:8s} ({segment['video_file']}): "
                               f"{segment_accuracy:.4f} ({segment_correct:3d}/{len(segment_results):3d})")
        
        # í˜¼ë™í–‰ë ¬ ë°ì´í„°
        self.logger.info("\nğŸ” í˜¼ë™ í–‰ë ¬ (ì˜ˆì¸¡ vs ì‹¤ì œ):")
        confusion_data = {}
        for gesture in self.gesture_classes.values():
            confusion_data[gesture] = {g: 0 for g in self.gesture_classes.values()}
        
        for result in results:
            if result['prediction'] and result['ground_truth']:
                confusion_data[result['ground_truth']][result['prediction']] += 1
        
        # í˜¼ë™í–‰ë ¬ ì¶œë ¥
        header = "     " + "".join([f"{g:8s}" for g in self.gesture_classes.values()])
        self.logger.info(header)
        for true_gesture in self.gesture_classes.values():
            row = f"{true_gesture:8s}:"
            for pred_gesture in self.gesture_classes.values():
                count = confusion_data[true_gesture][pred_gesture]
                row += f"{count:8d}"
            self.logger.info(row)
    
    def run_full_validation(self, videos_per_gesture: int = 2):
        """ì „ì²´ ê²€ì¦ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("ğŸ¯ ë°ëª¨ ì˜ìƒ ê²€ì¦ ì‹œì‘")
        self.logger.info("="*60)
        
        # 1. ëœë¤ ì˜ìƒ ì„ íƒ
        selected_videos = self.select_random_videos(videos_per_gesture)
        
        # 2. ë°ëª¨ ì˜ìƒ ìƒì„±
        demo_path, segments = self.create_concatenated_demo(selected_videos)
        
        # 3. ì‹¤ì‹œê°„ ê²€ì¦
        results, prediction_video = self.run_realtime_validation(demo_path, segments)
        
        self.logger.info("\nâœ… ê²€ì¦ ì™„ë£Œ!")
        self.logger.info(f"ğŸ“¹ ì›ë³¸ ë°ëª¨: {demo_path}")
        self.logger.info(f"ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼: {prediction_video}")
        
        return {
            'demo_video': demo_path,
            'prediction_video': prediction_video,
            'results': results,
            'segments': segments
        }

if __name__ == "__main__":
    print("ğŸ¯ ë°ëª¨ ì˜ìƒ ìƒì„± ë° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    creator = DemoVideoCreator()
    selected = creator.select_random_videos(videos_per_gesture=2)
    demo_path, segments = creator.create_concatenated_demo(selected)
    
    print(f"âœ… ë°ëª¨ ì˜ìƒ ìƒì„± ì™„ë£Œ: {demo_path}") 