# -*- coding: utf-8 -*-
"""
Improved Adaptive Window Gesture Detection
ì˜¤ë™ì‘ ë°©ì§€ë¥¼ ìœ„í•œ ê°œì„ ëœ ì ì‘í˜• ì œìŠ¤ì²˜ ì¸ì‹
"""

import cv2
import mediapipe as mp
import numpy as np
import torch
import os
from collections import deque
from typing import Optional, Tuple, List, Dict
import time

from config import DATA_CONFIG, MEDIAPIPE_CONFIG, GESTURE_CLASSES, TCN_CONFIG, IMPROVED_GESTURE_CONFIG
from model import GestureModelManager

class GestureTransitionDetector:
    """ì œìŠ¤ì²˜ ì „í™˜ íŒ¨í„´ ê°ì§€"""
    
    def __init__(self):
        self.transition_buffer = deque(maxlen=60)  # 2ì´ˆ ë²„í¼
        self.common_transitions = IMPROVED_GESTURE_CONFIG['common_transitions']
    
    def detect_transition(self, current_gesture, confidence):
        """ì œìŠ¤ì²˜ ì „í™˜ íŒ¨í„´ ê°ì§€"""
        if current_gesture is None:
            return False, None
            
        self.transition_buffer.append((current_gesture, confidence))
        
        if len(self.transition_buffer) < 20:
            return False, None
        
        # ìµœê·¼ 20í”„ë ˆì„ì—ì„œ ì „í™˜ íŒ¨í„´ ì°¾ê¸°
        recent_gestures = [g for g, c in list(self.transition_buffer)[-20:]]
        
        for pattern_name, pattern in self.common_transitions.items():
            if self._matches_pattern(recent_gestures, pattern):
                return True, pattern[-1]  # ìµœì¢… ì œìŠ¤ì²˜ ë°˜í™˜
        
        return False, None
    
    def _matches_pattern(self, gestures, pattern):
        """íŒ¨í„´ ë§¤ì¹­"""
        if len(gestures) < len(pattern):
            return False
        
        # ëì—ì„œë¶€í„° íŒ¨í„´ ë§¤ì¹­
        for i, expected in enumerate(reversed(pattern)):
            if gestures[-(i+1)] != expected:
                return False
        return True

class ImprovedAdaptiveWindowPoseDetector:
    """ê°œì„ ëœ ë‹¤ì¤‘ ìœˆë„ìš° í¬ê¸°ë¥¼ ì‚¬ìš©í•œ ì ì‘í˜• ì‹¤ì‹œê°„ í¬ì¦ˆ ê²€ì¶œ"""
    
    def __init__(self, model_path: str = None):
        # MediaPipe ì´ˆê¸°í™” (ê²½ê³  í•´ê²°ì„ ìœ„í•œ ê°œì„ ëœ ì„¤ì •)
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            enable_segmentation=False,
            smooth_landmarks=True,
            smooth_segmentation=True,
            min_detection_confidence=DATA_CONFIG['min_detection_confidence'],
            min_tracking_confidence=DATA_CONFIG['min_tracking_confidence']
        )
        
        self.mp_drawing = mp.solutions.drawing_utils
        
        # ê´€ì ˆ ì¸ë±ìŠ¤
        self.key_landmarks = MEDIAPIPE_CONFIG['key_landmarks']
        
        # ë‹¤ì¤‘ ìœˆë„ìš° ë²„í¼
        self.window_sizes = [30, 45, 60, 90]  # 1ì´ˆ, 1.5ì´ˆ, 2ì´ˆ, 3ì´ˆ
        self.pose_buffers = {
            size: deque(maxlen=size) for size in self.window_sizes
        }
        
        # ëª¨ë¸ ë¡œë“œ
        self.model_manager = GestureModelManager(model_path)
        self.model = self.model_manager.load_model()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì 
        self.prediction_results = {}
        self.confidence_weights = {
            30: 0.2,   # ì§§ì€ ìœˆë„ìš°ëŠ” ë‚®ì€ ê°€ì¤‘ì¹˜
            45: 0.3,   # ì¤‘ê°„ ìœˆë„ìš°
            60: 0.3,   # ì¤‘ê°„ ìœˆë„ìš°
            90: 0.2    # ê¸´ ìœˆë„ìš°ëŠ” ì•½ê°„ ë‚®ì€ ê°€ì¤‘ì¹˜ (ì§€ì—° ê³ ë ¤)
        }
        
        # ë™ì‘ ìƒíƒœ ì¶”ì 
        self.motion_state = {
            'is_moving': False,
            'motion_start_time': 0,
            'motion_intensity': 0.0,
            'stable_frames': 0
        }
        
        # ìµœì¢… ì˜ˆì¸¡
        self.final_prediction = None
        self.final_confidence = 0.0
        self.prediction_history = deque(maxlen=90)  # 3ì´ˆ ì´ë ¥
        self.confidence_history = deque(maxlen=30)  # 1ì´ˆ ì‹ ë¢°ë„ ì´ë ¥
        
        # ê°œì„ ëœ ê¸°ëŠ¥ë“¤
        self.transition_detector = GestureTransitionDetector()
        self.improved_config = IMPROVED_GESTURE_CONFIG
        
        # ì„±ëŠ¥ ì¶”ì 
        self.fps_counter = 0
        self.fps_start_time = time.time()
        
    def extract_pose_landmarks(self, frame: np.ndarray) -> Optional[np.ndarray]:
        """í”„ë ˆì„ì—ì„œ ìì„¸ ëœë“œë§ˆí¬ ì¶”ì¶œ"""
        # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´ ì„¤ì • (MediaPipe ê²½ê³  í•´ê²°)
        height, width = frame.shape[:2]
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        rgb_frame.flags.writeable = False
        
        results = self.pose.process(rgb_frame)
        
        if results.pose_landmarks:
            # 17ê°œ ì£¼ìš” ê´€ì ˆ ì¢Œí‘œ ì¶”ì¶œ
            pose_data = []
            for idx in self.key_landmarks:
                landmark = results.pose_landmarks.landmark[idx]
                pose_data.append([landmark.x, landmark.y, landmark.visibility])
            
            return np.array(pose_data, dtype=np.float32), results
        
        return None, results
    
    def normalize_pose_data(self, pose_data: np.ndarray) -> np.ndarray:
        """ìì„¸ ë°ì´í„° ì •ê·œí™”"""
        if pose_data.shape[0] == 0:
            return pose_data
            
        normalized_pose = pose_data.copy()
        
        # Hip ì¤‘ì‹¬ì  ê³„ì‚°
        left_hip = pose_data[9]   # left_hip
        right_hip = pose_data[10] # right_hip
        
        if left_hip[2] > 0.5 and right_hip[2] > 0.5:
            center = (left_hip[:2] + right_hip[:2]) / 2
            
            # ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜
            for joint_idx in range(len(self.key_landmarks)):
                if pose_data[joint_idx][2] > 0.5:
                    normalized_pose[joint_idx][:2] -= center
            
            # ìŠ¤ì¼€ì¼ ì •ê·œí™” (ì–´ê¹¨ ë„ˆë¹„ ê¸°ì¤€)
            left_shoulder = pose_data[3]   # left_shoulder
            right_shoulder = pose_data[4]  # right_shoulder
            
            if left_shoulder[2] > 0.5 and right_shoulder[2] > 0.5:
                shoulder_width = np.linalg.norm(left_shoulder[:2] - right_shoulder[:2])
                if shoulder_width > 0:
                    normalized_pose[:, :2] /= shoulder_width
        
        return normalized_pose
    
    def calculate_motion_intensity(self, pose_data: np.ndarray) -> float:
        """ë™ì‘ ê°•ë„ ê³„ì‚°"""
        if len(self.pose_buffers[30]) < 2:
            return 0.0
        
        # ìµœê·¼ ë‘ í”„ë ˆì„ ë¹„êµ
        prev_pose = list(self.pose_buffers[30])[-2]
        curr_pose = pose_data[:, :2]  # x, yë§Œ ì‚¬ìš©
        
        # ê´€ì ˆë³„ ì›€ì§ì„ ê³„ì‚°
        motion_per_joint = []
        for joint_idx in range(len(curr_pose)):
            if pose_data[joint_idx][2] > 0.5:  # visibility check
                motion = np.linalg.norm(curr_pose[joint_idx] - prev_pose[joint_idx])
                motion_per_joint.append(motion)
        
        return np.mean(motion_per_joint) if motion_per_joint else 0.0
    
    def update_motion_state(self, motion_intensity: float):
        """ë™ì‘ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        motion_threshold = 0.02  # ì›€ì§ì„ ì„ê³„ê°’
        stable_threshold = 10    # ì•ˆì • í”„ë ˆì„ ìˆ˜
        
        if motion_intensity > motion_threshold:
            if not self.motion_state['is_moving']:
                self.motion_state['motion_start_time'] = time.time()
                self.motion_state['is_moving'] = True
            self.motion_state['stable_frames'] = 0
        else:
            self.motion_state['stable_frames'] += 1
            if self.motion_state['stable_frames'] > stable_threshold:
                self.motion_state['is_moving'] = False
        
        self.motion_state['motion_intensity'] = motion_intensity
    
    def get_dynamic_threshold(self, motion_duration: float, motion_intensity: float) -> float:
        """ë™ì  ì‹ ë¢°ë„ ì„ê³„ê°’ ê³„ì‚°"""
        thresholds = self.improved_config['dynamic_thresholds']
        
        if motion_duration < 0.5:
            return thresholds['early_stage']
        elif motion_duration < 1.0:
            return thresholds['mid_stage']
        elif motion_duration < 2.0:
            return thresholds['late_stage']
        else:
            # ê¸´ ë™ì‘: ì•ˆì •ì„± ì¤‘ì‹œ
            if motion_intensity < self.improved_config['completion_motion_threshold']:
                return thresholds['completion']
            else:
                return thresholds['late_stage']
    
    def is_gesture_completed(self, motion_intensity: float, stable_frames: int, motion_duration: float) -> bool:
        """ì œìŠ¤ì²˜ ì™„ë£Œ ì—¬ë¶€ íŒë‹¨"""
        return (
            motion_intensity < self.improved_config['completion_motion_threshold'] and
            stable_frames > self.improved_config['completion_stable_frames'] and
            motion_duration > self.improved_config['min_motion_duration']
        )
    
    def analyze_prediction_consistency(self, recent_predictions: List[Tuple[str, float]]) -> Tuple[Optional[str], float]:
        """ìµœê·¼ ì˜ˆì¸¡ë“¤ì˜ ì¼ê´€ì„± ë¶„ì„"""
        window_frames = self.improved_config['consistency_window_frames']
        
        if len(recent_predictions) < window_frames:
            return None, 0.0
        
        # ë§ˆì§€ë§‰ Ní”„ë ˆì„ ë¶„ì„
        last_frames = recent_predictions[-window_frames:]
        
        # ê° ì œìŠ¤ì²˜ë³„ ë¹ˆë„ ê³„ì‚°
        gesture_counts = {}
        for gesture, conf in last_frames:
            if gesture not in gesture_counts:
                gesture_counts[gesture] = []
            gesture_counts[gesture].append(conf)
        
        # ê°€ì¥ ì¼ê´€ëœ ì œìŠ¤ì²˜ ì°¾ê¸°
        best_gesture = None
        best_consistency = 0.0
        
        for gesture, confidences in gesture_counts.items():
            required_frames = int(window_frames * 0.7)  # 70% ì´ìƒ
            if len(confidences) >= required_frames:
                consistency = len(confidences) / window_frames * np.mean(confidences)
                if consistency > best_consistency:
                    best_gesture = gesture
                    best_consistency = consistency
        
        return best_gesture, best_consistency
    
    def analyze_confidence_trend(self, recent_confidences: List[float]) -> Tuple[bool, float]:
        """ì‹ ë¢°ë„ ë³€í™” ì¶”ì„¸ ë¶„ì„"""
        if len(recent_confidences) < 10:
            return False, 0.0
        
        # ìµœê·¼ 10ê°œ ì‹ ë¢°ë„ ê°’
        confidences = recent_confidences[-10:]
        
        # ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ ê³„ì‚°
        x = np.arange(len(confidences))
        slope = np.polyfit(x, confidences, 1)[0]
        
        # ì¶”ì„¸ íŒë‹¨
        is_increasing = slope > self.improved_config['confidence_gradient_min']
        stability = 1.0 - np.std(confidences)  # ì•ˆì •ì„±
        
        return is_increasing and stability > 0.8, slope
    
    def smart_window_selection(self, motion_duration: float, gesture_history: List[Tuple[str, float]]) -> List[int]:
        """ìŠ¤ë§ˆíŠ¸ ìœˆë„ìš° ì„ íƒ ì „ëµ"""
        if not self.improved_config['smart_window_selection']:
            return self.window_sizes
        
        # ì œìŠ¤ì²˜ë³„ ìµœì  ìœˆë„ìš° í¬ê¸°
        gesture_optimal_windows = self.improved_config['gesture_optimal_windows']
        
        # ìµœê·¼ ì œìŠ¤ì²˜ ì´ë ¥ ê¸°ë°˜ ì„ íƒ
        if gesture_history:
            recent_gesture = gesture_history[-1][0]
            optimal_windows = gesture_optimal_windows.get(recent_gesture, [45, 60])
        else:
            optimal_windows = [45, 60]  # ê¸°ë³¸ê°’
        
        # ë™ì‘ ì§€ì†ì‹œê°„ ê³ ë ¤
        if motion_duration < 1.0:
            return [w for w in optimal_windows if w <= 45]
        elif motion_duration > 2.0:
            return [w for w in optimal_windows if w >= 60]
        else:
            return optimal_windows
    
    def predict_with_window(self, window_size: int) -> Tuple[Optional[str], float]:
        """íŠ¹ì • ìœˆë„ìš° í¬ê¸°ë¡œ ì œìŠ¤ì²˜ ì˜ˆì¸¡"""
        if len(self.pose_buffers[window_size]) < window_size or self.model is None:
            return None, 0.0
        
        # ë²„í¼ë¥¼ í…ì„œë¡œ ë³€í™˜
        pose_sequence = np.array(list(self.pose_buffers[window_size]))  # (window_size, 17, 2)
        pose_sequence = pose_sequence.reshape(pose_sequence.shape[0], -1)  # (window_size, 34)
        
        # 30í”„ë ˆì„ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§ (ëª¨ë¸ì€ 30í”„ë ˆì„ìœ¼ë¡œ í•™ìŠµë¨)
        if window_size != 30:
            indices = np.linspace(0, len(pose_sequence) - 1, 30, dtype=int)
            pose_sequence = pose_sequence[indices]
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        input_tensor = torch.FloatTensor(pose_sequence).unsqueeze(0).to(self.device)  # (1, 30, 34)
        
        # ì˜ˆì¸¡
        predictions, confidences, probabilities = self.model.predict(input_tensor)
        
        predicted_class = predictions[0].item()
        confidence = confidences[0].item()
        
        gesture_name = GESTURE_CLASSES[predicted_class]
        return gesture_name, confidence
    
    def improved_adaptive_prediction(self, motion_duration: float) -> Tuple[Optional[str], float, Dict]:
        """ê°œì„ ëœ ì ì‘í˜• ì˜ˆì¸¡"""
        debug_info = {
            'selected_windows': [],
            'window_predictions': {},
            'consistency_info': {},
            'confidence_trend': {},
            'transition_detected': False
        }
        
        # ìŠ¤ë§ˆíŠ¸ ìœˆë„ìš° ì„ íƒ
        selected_windows = self.smart_window_selection(motion_duration, list(self.prediction_history))
        debug_info['selected_windows'] = selected_windows
        
        # ë™ì  ì„ê³„ê°’ ê³„ì‚°
        dynamic_threshold = self.get_dynamic_threshold(motion_duration, self.motion_state['motion_intensity'])
        
        predictions = {}
        total_weight = 0
        
        # ì„ íƒëœ ìœˆë„ìš°ë“¤ë¡œ ì˜ˆì¸¡
        for window_size in selected_windows:
            gesture, confidence = self.predict_with_window(window_size)
            debug_info['window_predictions'][f'{window_size}f'] = f'{gesture}({confidence:.2f})' if gesture else 'None'
            
            if gesture and confidence > dynamic_threshold:
                weight = self.confidence_weights[window_size] * confidence
                
                if gesture not in predictions:
                    predictions[gesture] = 0
                predictions[gesture] += weight
                total_weight += weight
        
        if not predictions:
            return None, 0.0, debug_info
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡
        best_gesture = max(predictions.keys(), key=lambda x: predictions[x])
        final_confidence = predictions[best_gesture] / total_weight if total_weight > 0 else 0.0
        
        # ì˜ˆì¸¡ ì¼ê´€ì„± ë¶„ì„
        if len(self.prediction_history) >= 30:
            consistent_gesture, consistency = self.analyze_prediction_consistency(list(self.prediction_history))
            debug_info['consistency_info'] = {
                'consistent_gesture': consistent_gesture,
                'consistency_score': consistency
            }
            
            # ì¼ê´€ì„±ì´ ì¢‹ìœ¼ë©´ í•´ë‹¹ ì œìŠ¤ì²˜ ìš°ì„ 
            if (consistent_gesture and 
                consistency > self.improved_config['consistency_threshold'] and
                consistent_gesture in predictions):
                best_gesture = consistent_gesture
                final_confidence = max(final_confidence, consistency)
        
        # ì‹ ë¢°ë„ ì¶”ì„¸ ë¶„ì„
        if len(self.confidence_history) >= 10:
            is_stable_increasing, trend = self.analyze_confidence_trend(list(self.confidence_history))
            debug_info['confidence_trend'] = {
                'is_stable_increasing': is_stable_increasing,
                'trend_slope': trend
            }
            
            # ì¶”ì„¸ê°€ ì¢‹ì§€ ì•Šìœ¼ë©´ ì‹ ë¢°ë„ ê°ì†Œ
            if not is_stable_increasing:
                final_confidence *= 0.9
        
        # ì œìŠ¤ì²˜ ì „í™˜ ê°ì§€
        if self.improved_config['transition_detection']:
            transition_detected, transition_gesture = self.transition_detector.detect_transition(best_gesture, final_confidence)
            debug_info['transition_detected'] = transition_detected
            
            if transition_detected and transition_gesture:
                best_gesture = transition_gesture
                final_confidence = min(final_confidence * 1.1, 1.0)  # ì•½ê°„ ë³´ì •
        
        return best_gesture, final_confidence, debug_info 
    
    def process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, Optional[str], float, Dict]:
        """ê°œì„ ëœ í”„ë ˆì„ ì²˜ë¦¬ ë° ì ì‘í˜• ì œìŠ¤ì²˜ ì¸ì‹"""
        # ìì„¸ ì¶”ì¶œ
        pose_result = self.extract_pose_landmarks(frame)
        
        debug_info = {
            'motion_intensity': 0.0,
            'motion_state': 'idle',
            'window_predictions': {},
            'final_method': 'improved_adaptive',
            'dynamic_threshold': 0.75,
            'gesture_completed': False,
            'consistency_info': {},
            'confidence_trend': {}
        }
        
        if pose_result[0] is not None:
            pose_data, results = pose_result
            
            # ì •ê·œí™”
            normalized_pose = self.normalize_pose_data(pose_data)
            
            # ëª¨ë“  ìœˆë„ìš° ë²„í¼ì— ì¶”ê°€ (x, y ì¢Œí‘œë§Œ)
            pose_xy = normalized_pose[:, :2]
            for window_size in self.window_sizes:
                self.pose_buffers[window_size].append(pose_xy)
            
            # ë™ì‘ ê°•ë„ ê³„ì‚° ë° ìƒíƒœ ì—…ë°ì´íŠ¸
            motion_intensity = self.calculate_motion_intensity(normalized_pose)
            self.update_motion_state(motion_intensity)
            
            # ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸°
            annotated_frame = frame.copy()
            self.mp_drawing.draw_landmarks(
                annotated_frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS)
            
            # ë™ì‘ ì§€ì†ì‹œê°„ ê³„ì‚°
            motion_duration = time.time() - self.motion_state['motion_start_time']
            
            # ê°œì„ ëœ ì ì‘í˜• ì˜ˆì¸¡
            gesture, confidence, pred_debug_info = self.improved_adaptive_prediction(motion_duration)
            
            # ë””ë²„ê·¸ ì •ë³´ ë³‘í•©
            debug_info.update(pred_debug_info)
            debug_info.update({
                'motion_intensity': motion_intensity,
                'motion_state': 'moving' if self.motion_state['is_moving'] else 'stable',
                'motion_duration': motion_duration,
                'dynamic_threshold': self.get_dynamic_threshold(motion_duration, motion_intensity)
            })
            
            # ì œìŠ¤ì²˜ ì™„ë£Œ ì—¬ë¶€ í™•ì¸
            gesture_completed = self.is_gesture_completed(
                motion_intensity, 
                self.motion_state['stable_frames'], 
                motion_duration
            )
            debug_info['gesture_completed'] = gesture_completed
            
            # ì˜ˆì¸¡ ì´ë ¥ ê´€ë¦¬
            if gesture and confidence > 0.6:
                self.prediction_history.append((gesture, confidence))
                self.confidence_history.append(confidence)
                
                # ìµœì¢… ì˜ˆì¸¡ ì—…ë°ì´íŠ¸ (ì™„ë£Œëœ ë™ì‘ë§Œ)
                if gesture_completed or motion_duration > 2.0:
                    self.final_prediction = gesture
                    self.final_confidence = confidence
            
            return annotated_frame, self.final_prediction, self.final_confidence, debug_info
        
        return frame, self.final_prediction, self.final_confidence, debug_info
    
    def draw_adaptive_info(self, frame: np.ndarray, gesture: Optional[str], confidence: float, debug_info: Dict) -> np.ndarray:
        """ê°œì„ ëœ ì ì‘í˜• ì •ë³´ ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°"""
        height, width = frame.shape[:2]
        
        # FPS ê³„ì‚°
        self.fps_counter += 1
        if time.time() - self.fps_start_time >= 1.0:
            fps = self.fps_counter / (time.time() - self.fps_start_time)
            self.fps_counter = 0
            self.fps_start_time = time.time()
            self.current_fps = fps
        
        # ê¸°ë³¸ ì •ë³´
        info_text = [
            f"FPS: {getattr(self, 'current_fps', 0):.1f}",
            f"Motion: {debug_info['motion_state']} ({debug_info['motion_intensity']:.3f})",
            f"Duration: {debug_info.get('motion_duration', 0):.1f}s",
            f"Threshold: {debug_info.get('dynamic_threshold', 0.75):.2f}",
            f"Completed: {debug_info.get('gesture_completed', False)}"
        ]
        
        # ìµœì¢… ì˜ˆì¸¡
        if gesture:
            info_text.extend([
                f"Gesture: {gesture}",
                f"Confidence: {confidence:.2f}"
            ])
        
        # ì„ íƒëœ ìœˆë„ìš° ì •ë³´
        selected_windows = debug_info.get('selected_windows', [])
        window_info = [f"Selected: {selected_windows}"]
        
        # ìœˆë„ìš°ë³„ ì˜ˆì¸¡ (ì‘ì€ ê¸€ì”¨ë¡œ)
        for window, pred in debug_info.get('window_predictions', {}).items():
            window_info.append(f"{window}: {pred}")
        
        # ì¼ê´€ì„± ì •ë³´
        consistency_info = debug_info.get('consistency_info', {})
        if consistency_info:
            window_info.append(f"Consistent: {consistency_info.get('consistent_gesture', 'None')}")
            window_info.append(f"Score: {consistency_info.get('consistency_score', 0):.2f}")
        
        # ì‹ ë¢°ë„ ì¶”ì„¸ ì •ë³´
        trend_info = debug_info.get('confidence_trend', {})
        if trend_info:
            window_info.append(f"Trend: {trend_info.get('is_stable_increasing', False)}")
            window_info.append(f"Slope: {trend_info.get('trend_slope', 0):.3f}")
        
        # ì „í™˜ ê°ì§€ ì •ë³´
        if debug_info.get('transition_detected', False):
            window_info.append("Transition: YES")
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (ì¢Œì¸¡)
        y_offset = 30
        for text in info_text:
            cv2.putText(frame, text, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                       0.6, (0, 255, 0), 2)
            y_offset += 25
        
        # ìœˆë„ìš° ì •ë³´ (ìš°ì¸¡)
        y_offset = 30
        for text in window_info:
            cv2.putText(frame, text, (width-300, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                       0.4, (255, 255, 0), 1)
            y_offset += 20
        
        # ì œìŠ¤ì²˜ ìƒíƒœ í‘œì‹œ
        if gesture and confidence > 0.8:
            # ì™„ë£Œëœ ì œìŠ¤ì²˜ëŠ” ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ë¡œ ê°•ì¡°
            color = (0, 0, 255) if debug_info.get('gesture_completed', False) else (0, 255, 255)
            cv2.rectangle(frame, (width//2-100, 10), (width//2+100, 80), color, 3)
            cv2.putText(frame, gesture, (width//2-80, 40), cv2.FONT_HERSHEY_SIMPLEX, 
                       1.0, color, 3)
            cv2.putText(frame, f"{confidence:.2f}", (width//2-50, 65), cv2.FONT_HERSHEY_SIMPLEX, 
                       0.7, color, 2)
            
            # ì™„ë£Œ ìƒíƒœ í‘œì‹œ
            if debug_info.get('gesture_completed', False):
                cv2.putText(frame, "COMPLETED", (width//2-60, 95), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.5, (0, 0, 255), 2)
        
        return frame
    
    def run_camera(self, camera_index: int = 0):
        """ê°œì„ ëœ ì ì‘í˜• ì¹´ë©”ë¼ ì‹¤ì‹œê°„ ì²˜ë¦¬"""
        cap = cv2.VideoCapture(camera_index)
        
        if not cap.isOpened():
            print(f"ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {camera_index}")
            return
        
        print("ğŸ¯ ê°œì„ ëœ ì ì‘í˜• ì œìŠ¤ì²˜ ì¸ì‹ ì‹œì‘ (q í‚¤ë¡œ ì¢…ë£Œ)")
        print("ğŸ“Š ë‹¤ì¤‘ ìœˆë„ìš°: 30f(1s), 45f(1.5s), 60f(2s), 90f(3s)")
        print("ğŸ§  ìŠ¤ë§ˆíŠ¸ ìœˆë„ìš° ì„ íƒ: í™œì„±í™”")
        print("ğŸ” ë™ì  ì„ê³„ê°’: 0.95 â†’ 0.75")
        print("âœ… ë™ì‘ ì™„ë£Œ ê°ì§€: í™œì„±í™”")
        print("ğŸ”„ ì „í™˜ íŒ¨í„´ ê°ì§€: í™œì„±í™”")
        print("ğŸ“ˆ ì¼ê´€ì„± ë¶„ì„: í™œì„±í™”")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # í”„ë ˆì„ ì²˜ë¦¬
            processed_frame, gesture, confidence, debug_info = self.process_frame(frame)
            
            # ì •ë³´ ì˜¤ë²„ë ˆì´
            final_frame = self.draw_adaptive_info(processed_frame, gesture, confidence, debug_info)
            
            # í™”ë©´ ì¶œë ¥
            cv2.imshow('Improved PDS - Advanced Gesture Recognition', final_frame)
            
            # ì¢…ë£Œ ì¡°ê±´
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    # ê°œì„ ëœ ì ì‘í˜• ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸
    detector = ImprovedAdaptiveWindowPoseDetector()
    detector.run_camera() 