#!/usr/bin/env python3
"""
GPU ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import gc
import os

def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬"""
    print("=== GPU ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬ ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í˜„ì¬ ìƒíƒœ í™•ì¸
    allocated = torch.cuda.memory_allocated() / 1024**3
    cached = torch.cuda.memory_reserved() / 1024**3
    total = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    print(f"ì •ë¦¬ ì „ GPU ë©”ëª¨ë¦¬:")
    print(f"  í• ë‹¹ë¨: {allocated:.2f}GB")
    print(f"  ìºì‹œë¨: {cached:.2f}GB")
    print(f"  ì´ ìš©ëŸ‰: {total:.2f}GB")
    print(f"  ì‚¬ìš©ë¥ : {(allocated + cached)/total*100:.1f}%")
    
    # 1ë‹¨ê³„: PyTorch ìºì‹œ ì •ë¦¬
    print("\n1. PyTorch ìºì‹œ ì •ë¦¬...")
    torch.cuda.empty_cache()
    
    # 2ë‹¨ê³„: Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    print("2. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜...")
    gc.collect()
    
    # 3ë‹¨ê³„: ë©”ëª¨ë¦¬ ë¶„í•  í•´ì œ
    print("3. ë©”ëª¨ë¦¬ ë¶„í•  í•´ì œ...")
    torch.cuda.memory.empty_cache()
    
    # 4ë‹¨ê³„: ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ì¬ì„¤ì •
    print("4. ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ì¬ì„¤ì •...")
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'
    
    # 5ë‹¨ê³„: ë©”ëª¨ë¦¬ ì œí•œ í•´ì œ í›„ ì¬ì„¤ì •
    print("5. ë©”ëª¨ë¦¬ ì œí•œ ì¬ì„¤ì •...")
    torch.cuda.set_per_process_memory_fraction(0.95)  # 95%ë¡œ ì„¤ì •
    
    # ì •ë¦¬ í›„ ìƒíƒœ í™•ì¸
    allocated_after = torch.cuda.memory_allocated() / 1024**3
    cached_after = torch.cuda.memory_reserved() / 1024**3
    
    print(f"\nì •ë¦¬ í›„ GPU ë©”ëª¨ë¦¬:")
    print(f"  í• ë‹¹ë¨: {allocated_after:.2f}GB")
    print(f"  ìºì‹œë¨: {cached_after:.2f}GB")
    print(f"  ì‚¬ìš©ë¥ : {(allocated_after + cached_after)/total*100:.1f}%")
    print(f"  ì‚¬ìš© ê°€ëŠ¥: {total - allocated_after - cached_after:.2f}GB")
    
    # ì •ë¦¬ íš¨ê³¼
    freed_memory = (allocated + cached) - (allocated_after + cached_after)
    print(f"\nâœ… {freed_memory:.2f}GB ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    if freed_memory > 4.0:
        print("ğŸ† Large ëª¨ë¸ ì‹¤í–‰ ê°€ëŠ¥!")
    elif freed_memory > 2.0:
        print("ğŸš€ Medium ëª¨ë¸ ì‹¤í–‰ ê°€ëŠ¥!")
    else:
        print("âš ï¸ ì¶”ê°€ ì •ë¦¬ í•„ìš”")

def check_gpu_processes():
    """GPU ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ í™•ì¸"""
    print("\n=== GPU ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ í™•ì¸ ===")
    os.system("nvidia-smi")

if __name__ == "__main__":
    clear_gpu_memory()
    check_gpu_processes() 